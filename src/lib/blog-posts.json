[
    {
        "title": "Overcoming Sarcasm in Financial Headlines",
        "excerpt": "The challenge of detecting tonal shifts in market news. Using transformer attention mechanisms to distinguish cynicism from optimism.",
        "date": "Dec 15, 2025",
        "category": "Financial Analysis",
        "author": "Arghadeep Saha",
        "slug": "overcoming-sarcasm-financial-headlines",
        "content": "## Abstract\n\nIn the present time, where everything is moving forward at a rapid pace, identifying proper sentiments in the field of finance is becoming a big challenge for the people, especially those who are engaged in the stock market. It is not challenging due to insufficient data, but because of the language patterns that has been used which the present existing traditional models fail to clarify in a proper and in an accurate manner. Among these challenges, ironic tone and sarcasm are the most problematic part. This article explores why the expressions which are sarcastic in nature are so much common in financial headlines and how such expressions mislead the conventional systems which are based on the sentiment analysis, and also why the mechanism of transformer -based attention allow more reliable detection of ironic market signals.\n\n## Introduction\n\nFinancial headlines or tags are made in order to use them to capture the attention of people instantly. In an ecosystem where a large number of news are present, writers often depend on the tone that has been used. They do not rely on the direct statements, that has been given, to convey deeper knowledge. **Sarcasms**, *ironies* etc are employed frequently to question narratives of the corporate, policy announcements, or overly optimistic market scenes.\n\nHuman readers rarely struggle with these problems. But experienced traders, who are associated in this field for a long period of time, can easily sense when a news headline is mocking overconfident predictions or questioning corporate optimism. For the systems which are automated, however, sarcasm remains one of the big challenges in area of natural language processing.\n\nFor the contexts which are related to financial sentimental analysis, this problem carries real risk. A sarcastic statement or headline which seems to be positive on the surface may actually reflect negative sentiment. When the existing models fail to recognize this, they may create signals that contradict the actual behaviour of market.\n\nThis article tells the role of sarcasm in field of financial journalism, explains why traditional models, which are based on sentiments, struggle with it, and discusses how architectures which are transformer based help address this issue.\n\n## The Role of Sarcasm in Financial News\n\nSarcasm acts as a deliberate function in the field of financial writing. Journalists use it in order to express skepticism for avoiding explicit accusations. It allows one to criticise forecasts, repeated failures etc without damaging the sense of professional neutrality. For example praising a struggling company in an enthusiastic manner may leads to creation of doubts rather than confidence. When markets are continuously exposed to the promises which they made but fail to fulfil, sarcasm becomes the natural way to show frustration. It shows emotional context also. During period of downturns, headlines may sound reassuring while subtly hinting at instability. It is easier for humans to understand this but it is not same in the case of machines. They fail to decode it.\n\n### Why Traditional Sentiment Models Fall Short\n\nThe pre-existing conventional systems mostly depend on surface level indicators. They mostly count the positive and negative words or provide a fixed score of sentiment which is based on predefined dictionaries. If number of positive terms are more, then sentiment is positive and vice versa. Sarcasm disrupts this concept. In this, the sentence which seems to be filled with positive words may actually generate disbelief, criticism or frustration. Rule based systems cannot capture such contradiction. Even ML based models fail because sarcasm mostly depends on context, contrast and expectation rather than on vocabulary alone. In the domain of finance, without proper understanding of context, models misinterpret the sentiment and produce wrong outputs.\n\n## The Impact of Misreading Sarcasm\n\nIn the field of stock market, errors in detection of sarcasm are not trivial. A sarcastic headline incorrectly classified as bullish can trigger signals of buying during the wrong time. Repeated errors of such nature reduce confidence in the sentiment – driven strategies and introduce a lot of chaos during the time of decision making processes.\n\n### Example Logic in JS\n\nHere is how a simple sentiment check might fail:\n\n```javascript\nfunction checkSentiment(text) {\n  const positiveWords = ['growth', 'soar', 'high'];\n  let score = 0;\n  \n  text.split(' ').forEach(word => {\n    if (positiveWords.includes(word)) score++;\n  });\n  \n  // Returns positive for \"Great growth... NOT!\"\n  return score > 0 ? 'Bullish' : 'Bearish';\n}\n```\n\n## Role of Tonal Shifts\n\nSarcasm is less about words as its core is completely based on a tonal construct. It occurs due to the mismatch between the expectations and the expressions. For detecting the sarcasm, it is required to analyse how various parts of a sentence relate to each other. Various patterns like exaggerated praise followed by cautious qualifiers often indicate irony sense. It is difficult for the models to investigate such relationships but become evident when structure of the sentence and the context of the given topic are considered overall. For recognizing the tonal shifts, it is needed that the models must be capable of capturing the long range dependencies and sentence level interactions.\n\n- **Contrast**: Optimistic words with pessimistic context.\n- **Exaggeration**: Over-the-top praise that feels fake.\n- **Context**: Historical failures vs current promises.\n\n## Why Transformers are More Advantageous\n\nThe transformer based prototype are designed in such a way that it can easily model the relationships across the entire sentence in one go. Because of their various mechanisms, they evaluate how every word and phrase influence one another rather isolating the processing text. It allows them to find the contrast and the internal inconsistency. When the optimistic language clashes with the contextual cues, it can infer negative sentiment. Regarding the financial headlines, this capability permits the models to differ genuine confidence from rhetorical optimism that creates concern which the pre existing approaches fail to achieve.\n\n## Attention as a Feature for Extracting the Actual Meaning\n\nAttention mechanisms help in assigning greater importance to the words or phrases that excessively shape the meaning. In the sarcastic content, these are mostly modifiers or contextual references rather than the undisguised sentiment words. By giving focus on such segments, models will start to interpret tone. With time, this will help in getting more accurate results of identification. For the financial text part, attention driven modelling is very much effective at separating the authentic optimism from positivity that seems to appear convincing on the surface but indicates doubt underneath.\n\n## Challenges in Making Sarcasm Aware Models\n\nDespite the presence of advanced features, transformer models are not the complete solution for the problem of sarcasm. Sarcasm is completely subjective and it is influenced by the contexts which are based on cultural and market specific. Language or tone that seems to be ironic in nature in one economic phase may appear sincere in other phrases. Another challenge is the availability of proper data. Direct sarcasm labels are limited, and presence of ironic tone in financial based news is rarely annotated in raw datasets.\n\n## Conclusion\n\nSarcasm in headlines, which are financial based, is not a noise but a meaningful information passed through tone. By ignoring it, a lot of sentiment errors and unreliable decision-making will arise. Pre-existing traditional models struggle very much because they depend only on individual words rather than the whole context. Transformer based attention mechanism address this problem by capturing the contrast and intent, allowing the systems to differentiate genuine optimism from skepticism. As markets are continuously becoming narrative-driven, accurate tone interpretation will play a core role in advancing financial sentiment analysis."
    },
    {
        "title": "Normalising financial data for clean analysis",
        "excerpt": "Due to scale differences and inconsistencies, analysis of financial data remains extremely difficult despite appearing in abundance. How raw financial data is transformed into reliable and inputs for meaningful and accurate analysis are explored in the given article.",
        "date": "Dec 24, 2025",
        "category": "Data Processing",
        "author": "Adrish Chakraborty",
        "slug": "normalising-financial-data-for-clean-analysis",
        "content": "### ABSTRACT:\n\nDue to scale differences and inconsistencies, analysis of financial data remains extremely difficult despite appearing in abundance. How raw financial data is transformed into reliable and inputs for meaningful and accurate analysis are explored in the given article.\n\n### INTRODUCTION:\n\nFinancial markets have been generating enormous volumes of data every second. The investment decisions and economic policies are continuously shaped by stock prices, interest rates, financial statements belonging to the corporate sector, trading volumes and investment decisions and economic policies which are continuously shaped by macroeconomic indicators. Despite providing unprecedented opportunities for insight, the abundant data also introduces further analytical challenges. Clean analysis is difficult without proper preprocessing because raw financial data is rarely uniform or directly comparable.\n\nData normalisation is one of the most important preprocessing steps in financial analytics. Dramatic differences in scale, unit and statistical behaviour are observed in financial variables. For example, financial ratios such as return on equity may exist on a smaller numerical scale but the revenue figures of the company may be measured in billions. Models can become biased towards large magnitude variables while analysing without adjustment, despite the actual importance of the variables.\n\nWhether financial data is transformed into a consistent or comparable format or not is ensured by normalisation. This allows more focus of the analytical models on meaningful relationships apart from numerical dominance. The importance of normalising financial data, raw datasets and their challenges, popular normalisation techniques and how financial analysis and decision-making are impacted by these methods are discussed in the given article.\n\n### IMBALANCE OF SCALE AND NATURE OF FINANCIAL DATA:\n\nFinancial datasets are characterized by intrinsic variability. Collection of market data, accounting figures and economic indicators happen from different sources which are further reported under different conventions. Wide variation of prices is observed across several asset, dramatic fluctuations of trading volumes are observed between securities and entirely different distributions are often followed by balance sheet items.\n\nProblems in both machine learning and statistical models arise from this kind of scale imbalance. The variables with smaller numerical ranges are often shadowed by the ones having large numerical ranges, despite the former having significant informational value. For instance, volatility or liquidity measures may be dominated by raw market capitalization which further leads to results which are asymmetric.\n\nThis imbalance is corrected by rescaling data into a common framework through normalisation. The appropriate contribution of the variable to analysis, which further improves accuracy, fairness and interpretability across financial models is ensured by it.\n\n### RAW FINANCIAL DATASETS AND ITS COMMON CHALLENGES:\n\nNoise and inconsistencies are frequently observed in raw financial data. Common issues include reporting delays, differences in currencies, values which are missing varying fiscal calendars. Sudden spikes caused by announcements of earning, unexpected economic events and regulatory changes are also included in market data.\n\nMajor challenges are posed by outliers. Averages and variances can be distorted, or analytical results can be misleading due to extreme price movements and unusually high trading volumes. Inflation, evolution of accounting standards and structural market changes affect financial data that can be utilized in the long run.\n\nThe reliability of analysis is significantly reduced by these issues without proper preparation of data and normalisation. These problems are mitigated by improving comparability across time periods and assets and stabilising distributions through clean, normalised data.\n\n### OVERVIEW OF FINANCIAL NORMALISATION TECHNIQUES:\n\nFinancial analysis involves the application of several normalisation techniques, each of which serve a different purpose. The values are rescaled into a fix range, typically between zero and one through the Min-max normalisation. This method helps analysts preserve relative differences while ensuring comparability.\n\nBased on mean and standard deviation, data can be transformed using Z score standardisation. The deviation of the values from historical norms are highlighted int this approach and it is also used in analysing returns, performance metrics and risk factors. To manage exponential growth and reduce skewness, market capitalization, price series and volume data often use logarithmic transformations. To limit the influence of extreme outliers, in some cases, robust scaling techniques are recommended.\n\nThe assumptions of the analytical model, the nature of the data and the objectives of the analysis depend on the techniques chosen.\n\n###  NORMALISATION’S ROLE IN FINANCIAL MODELING:\n\nAn important role is played by normalisation in financial modelling. The stability of regression coefficients is improved, and the interpretability of results is enhanced in statistical models. Convergence is sped up through normalised data which further removes disproportionately waiting, large scale features from the models of machine learning algorithms.\n\nFair comparison across assets having volatile profiles and different price levels is allowed by normalised inputs during portfolio construction. The consistent evaluation of behavioural indicators and transaction values are ensured by normalisation in fraud detection models and credit risk.\n\nBoth the robustness and accuracy of the financial models are enhanced by normalisation, which further increases their reliability in real world applications.\n\n### THREATS ASSOCIATED WITH IGNORING NORMALISATION:\n\nSerious analytical errors are consequences of failure to normalise data. Hidden biases in the input data can lead to the failure of the models in the real market despite the models performing apparently well while training. Unknowingly relying on distorted signals by decision makers may lead to scale dominance, not economic reality.\n\nFinancial losses and poor execution are consequences of rapid propagation of such errors, especially in automated, high frequency trading systems. Confidence in data driven strategies are reduced over time due to repeated in accuracies that undermine trust in analytical systems.\n\n### LIMITATIONS AND PRACTICAL CONSIDERATIONS:\n\nThoughtful application of normalisation is necessary despite it being essential. Particularly during periods of market stress, rapid changes in financial data distributions are observed over time. Normalised strategies working in stable conditions need to be adjusted especially during volatile phases.\n\nInterpretability is another important consideration. Results become difficult to be explained to stakeholders due to excessive transformation, especially in regulated environments where maintaining transparency is a big task. Mathematical precision with practical clarity must be balanced by analysts.\n\n### NORMALISED DATA IN MODERN FINANCIAL ANALYTICS AND ITS IMPORTANCE:\n\nThe need for consistent and clean data grows stronger as financial analytics become more advanced. Applications such as risk management, algorithmic trading, financially intelligent platforms and forecasting are supported by normalised data.\n\nThe quality of insights is improved, and better strategic decisions are supported by normalisation, which further enables fair comparisons across markets, time horizons and assets. Simply, normalisation is a bridge between meaningful financial understanding and raw data.\n\n### CONCLUSION:\n\nNormalisation of financial data is a core requirement for accurate and clean analysis, despite it being a secondary step. Scale differences, distortions and inconsistencies present in raw datasets can mislead analytical models if not properly dealt with. Analysts can reduce bias, improve model performance and discover genuine patterns with the data through appropriate normalisation. Effective data normalisation will remain a cornerstone of trustworthy and reliable financial analysis, as finance continues to evolve into a data-driven field."
    },
    {
        "title": "A Review of Introduction to Kolmogorov-Arnold Network(KAN)",
        "excerpt": "In the past 60 years Multi-Layer Perceptron(MLP) has helped us as the fundamental block of deep learning based on the approximation theorem. Though MLP suffered from a critical lacking of interpretability and require massive parameter often. This article explores Kolmogorov-Arnold Network(KAN) model...",
        "date": "Dec 25, 2025",
        "category": "Backend Engineering",
        "author": "Arpan Pal",
        "slug": "an-introduction-to-kolmogorov-arnold-network-kan",
        "content": "### Abstract\n\nIn the past 60 years Multi-Layer Perceptron(MLP) has helped us as the fundamental block of deep learning based on the approximation theorem. Though MLP suffered from a critical lacking of interpretability and require massive parameter often. This article explores Kolmogorov-Arnold Network(KAN) model which was proposed in 2024. This Model was proposed by Andrey Kolmogorov and Vladimir Arnold. Previously MLP placed Fixed Activation functions on nodes but KAN Place Learnable Activation functions on edges. Here we will analyse the mathematical difference between these two architectures by examining the use B-Splines on edge and weights and we will also discuss How KAN offered us promised path towards the \"White Box\" AI that is very much accurate and interpretable.\n\n---\n\n### Introduction\n\n**1. Stagnation of Perceptron**\nThese neural network is built on a mathematical assumption specifically that is the linear combination of Inputs Based on Fixed nonlinearity. In standard MLP, we define a neural computation as:\n$$y = \\sigma \\left( \\sum_{i=1}^n w_i x_i + b \\right)$$\n**Here:**\n*   $w_i$ Indicates Learnable Scalar weights.\n*   $\\sigma$ Indicates Fixed activation function.\n\nThese structure Took help From the activation function by making it a static Gatekeeper. This structure results In Black box while universally approximating where Specific contribution of features is calculated by layers of matrix multiplication.\n\n---\n\n**2. Mathematical Foundation**\nTo understand KAN We focused 2 fundamental mathematical theorem.\n*   **Universal Approximation Theorem (MLP Basis):** MLP Calculates that Sum of all nonlinear function can give an approximate result to any continuous functions.\n*   **Kolmogorov-Arnold Theorem (KAN Basis):** KAN states that any multivariate continuous function lie on a bounded domain can be represented as a finite composition of multiple univariate functions.\n$$f(\\mathbf{x}) = f(x_1, \\dots, x_n) = \\sum_{q=0}^{2n} \\Phi_q \\left( \\sum_{p=1}^n \\phi_{q,p}(x_p) \\right)$$\n**In this formula:**\n*   $\\phi_{q,p}$ are univariate functions (functions of a single variable).\n*   $\\Phi_q$ is the aggregation function.\n\n---\n\n### Architecture\nThe first innovation on KAN is by replacing the linear weight with a learnable non-linear function.\n*   **The Activation Age:** In MLP if an `age = X` and `Skills Scales = W`. Then in KAN an `age = X` and transforms it via a function $\\phi(x)$.\n*   **Basic Splines(B-Splines) Implementation:** We cannot learn from an arbitrary function directly from code, so we will approximate these functions by using B-Splines, which are piecewise polynomial functions defined by control points.\nThe learnable function $\\phi(x)$ is expressed as:\n$$\\phi(x) = \\sum_i c_i B_i(x)$$\n**Where:**\n*   $B_i(x)$ are the fixed basis functions (the spline shape).\n*   $c_i$ are the learnable coefficients (control points).\n\n---\n\n### Pseudocode\n```python\n# 2. The KAN Approach (Learnable Edges, Summing Nodes)\nclass KAN_Layer:\n    def forward(self, x):\n        # Step A: Apply Learnable Non-Linear Functions\n        # Instead of multiplying by a scalar 'W', we apply a function 'phi'\n        # 'phi' is built using B-Splines (basis functions)\n        \n        # logic: y = sum(phi(x))\n        \n        # We compute the shape of the function based on learnable coefficients\n        spline_basis = compute_b_splines(x)\n        phi_x = spline_basis * self.spline_coefficients\n        \n        # Step B: Summation\n        # The node simply sums up the incoming function results\n        y = sum(phi_x)\n        return y\n```\n\n---\n\n### Advantages\n1.  **White box interpretability:** We know that every edge is a univariate function that we can see exactly how one input variable affects the other output variable. In MLP Feature interactions are ignored in weight matrices, but KAN allows us to plot one spline shape on every connection. For example, if spline shapes look like a parabola we confirm it is a quadratic relationship; if it shapes like a sinusoidal then we confirm the relationship is periodic.\n2.  **Parameter efficiency:** In Symbolic Regression KAN helps by reaching lower loss values with fewer parameters than MLPs. A small KAN often discovers a physics law where a large MLP only poorly approximates the curve without understanding its structure.\n\n---\n\n### Conclusion\nKAN shows a paradigm shift from learning linear weights to learning functions by using the Kolmogorov-Arnold Theorem and B-Splines. KAN offers us a glimpse into the future of interpretable Artificial Intelligence. Though currently training slower than MLPs due to the complexity of calculating splines, they provide a bridge over the gap between deep learning and symbolic mathematics. For an AI engineer studying KAN will help rethink the atomic unit of neural computation."
    }
]