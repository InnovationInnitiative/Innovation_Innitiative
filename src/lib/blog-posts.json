[
    {
        "title": "Overcoming Sarcasm in Financial Headlines",
        "excerpt": "The challenge of detecting tonal shifts in market news. Using transformer attention mechanisms to distinguish cynicism from optimism.",
        "date": "Dec 15, 2025",
        "category": "Financial Analysis",
        "author": "Arghadeep Saha",
        "slug": "overcoming-sarcasm-financial-headlines",
        "content": "## Abstract\n\nIn the present time, where everything is moving forward at a rapid pace, identifying proper sentiments in the field of finance is becoming a big challenge for the people, especially those who are engaged in the stock market. It is not challenging due to insufficient data, but because of the language patterns that has been used which the present existing traditional models fail to clarify in a proper and in an accurate manner. Among these challenges, ironic tone and sarcasm are the most problematic part. This article explores why the expressions which are sarcastic in nature are so much common in financial headlines and how such expressions mislead the conventional systems which are based on the sentiment analysis, and also why the mechanism of transformer -based attention allow more reliable detection of ironic market signals.\n\n## Introduction\n\nFinancial headlines or tags are made in order to use them to capture the attention of people instantly. In an ecosystem where a large number of news are present, writers often depend on the tone that has been used. They do not rely on the direct statements, that has been given, to convey deeper knowledge. **Sarcasms**, *ironies* etc are employed frequently to question narratives of the corporate, policy announcements, or overly optimistic market scenes.\n\nHuman readers rarely struggle with these problems. But experienced traders, who are associated in this field for a long period of time, can easily sense when a news headline is mocking overconfident predictions or questioning corporate optimism. For the systems which are automated, however, sarcasm remains one of the big challenges in area of natural language processing.\n\nFor the contexts which are related to financial sentimental analysis, this problem carries real risk. A sarcastic statement or headline which seems to be positive on the surface may actually reflect negative sentiment. When the existing models fail to recognize this, they may create signals that contradict the actual behaviour of market.\n\nThis article tells the role of sarcasm in field of financial journalism, explains why traditional models, which are based on sentiments, struggle with it, and discusses how architectures which are transformer based help address this issue.\n\n## The Role of Sarcasm in Financial News\n\nSarcasm acts as a deliberate function in the field of financial writing. Journalists use it in order to express skepticism for avoiding explicit accusations. It allows one to criticise forecasts, repeated failures etc without damaging the sense of professional neutrality. For example praising a struggling company in an enthusiastic manner may leads to creation of doubts rather than confidence. When markets are continuously exposed to the promises which they made but fail to fulfil, sarcasm becomes the natural way to show frustration. It shows emotional context also. During period of downturns, headlines may sound reassuring while subtly hinting at instability. It is easier for humans to understand this but it is not same in the case of machines. They fail to decode it.\n\n### Why Traditional Sentiment Models Fall Short\n\nThe pre-existing conventional systems mostly depend on surface level indicators. They mostly count the positive and negative words or provide a fixed score of sentiment which is based on predefined dictionaries. If number of positive terms are more, then sentiment is positive and vice versa. Sarcasm disrupts this concept. In this, the sentence which seems to be filled with positive words may actually generate disbelief, criticism or frustration. Rule based systems cannot capture such contradiction. Even ML based models fail because sarcasm mostly depends on context, contrast and expectation rather than on vocabulary alone. In the domain of finance, without proper understanding of context, models misinterpret the sentiment and produce wrong outputs.\n\n## The Impact of Misreading Sarcasm\n\nIn the field of stock market, errors in detection of sarcasm are not trivial. A sarcastic headline incorrectly classified as bullish can trigger signals of buying during the wrong time. Repeated errors of such nature reduce confidence in the sentiment – driven strategies and introduce a lot of chaos during the time of decision making processes.\n\n### Example Logic in JS\n\nHere is how a simple sentiment check might fail:\n\n```javascript\nfunction checkSentiment(text) {\n  const positiveWords = ['growth', 'soar', 'high'];\n  let score = 0;\n  \n  text.split(' ').forEach(word => {\n    if (positiveWords.includes(word)) score++;\n  });\n  \n  // Returns positive for \"Great growth... NOT!\"\n  return score > 0 ? 'Bullish' : 'Bearish';\n}\n```\n\n## Role of Tonal Shifts\n\nSarcasm is less about words as its core is completely based on a tonal construct. It occurs due to the mismatch between the expectations and the expressions. For detecting the sarcasm, it is required to analyse how various parts of a sentence relate to each other. Various patterns like exaggerated praise followed by cautious qualifiers often indicate irony sense. It is difficult for the models to investigate such relationships but become evident when structure of the sentence and the context of the given topic are considered overall. For recognizing the tonal shifts, it is needed that the models must be capable of capturing the long range dependencies and sentence level interactions.\n\n- **Contrast**: Optimistic words with pessimistic context.\n- **Exaggeration**: Over-the-top praise that feels fake.\n- **Context**: Historical failures vs current promises.\n\n## Why Transformers are More Advantageous\n\nThe transformer based prototype are designed in such a way that it can easily model the relationships across the entire sentence in one go. Because of their various mechanisms, they evaluate how every word and phrase influence one another rather isolating the processing text. It allows them to find the contrast and the internal inconsistency. When the optimistic language clashes with the contextual cues, it can infer negative sentiment. Regarding the financial headlines, this capability permits the models to differ genuine confidence from rhetorical optimism that creates concern which the pre existing approaches fail to achieve.\n\n## Attention as a Feature for Extracting the Actual Meaning\n\nAttention mechanisms help in assigning greater importance to the words or phrases that excessively shape the meaning. In the sarcastic content, these are mostly modifiers or contextual references rather than the undisguised sentiment words. By giving focus on such segments, models will start to interpret tone. With time, this will help in getting more accurate results of identification. For the financial text part, attention driven modelling is very much effective at separating the authentic optimism from positivity that seems to appear convincing on the surface but indicates doubt underneath.\n\n## Challenges in Making Sarcasm Aware Models\n\nDespite the presence of advanced features, transformer models are not the complete solution for the problem of sarcasm. Sarcasm is completely subjective and it is influenced by the contexts which are based on cultural and market specific. Language or tone that seems to be ironic in nature in one economic phase may appear sincere in other phrases. Another challenge is the availability of proper data. Direct sarcasm labels are limited, and presence of ironic tone in financial based news is rarely annotated in raw datasets.\n\n## Conclusion\n\nSarcasm in headlines, which are financial based, is not a noise but a meaningful information passed through tone. By ignoring it, a lot of sentiment errors and unreliable decision-making will arise. Pre-existing traditional models struggle very much because they depend only on individual words rather than the whole context. Transformer based attention mechanism address this problem by capturing the contrast and intent, allowing the systems to differentiate genuine optimism from skepticism. As markets are continuously becoming narrative-driven, accurate tone interpretation will play a core role in advancing financial sentiment analysis."
    },
    {
        "title": "Normalising financial data for clean analysis",
        "excerpt": "Due to scale differences and inconsistencies, analysis of financial data remains extremely difficult despite appearing in abundance. How raw financial data is transformed into reliable and inputs for meaningful and accurate analysis are explored in the given article.",
        "date": "Dec 24, 2025",
        "category": "Data Processing",
        "author": "Adrish Chakraborty",
        "slug": "normalising-financial-data-for-clean-analysis",
        "content": "### ABSTRACT:\n\nDue to scale differences and inconsistencies, analysis of financial data remains extremely difficult despite appearing in abundance. How raw financial data is transformed into reliable and inputs for meaningful and accurate analysis are explored in the given article.\n\n### INTRODUCTION:\n\nFinancial markets have been generating enormous volumes of data every second. The investment decisions and economic policies are continuously shaped by stock prices, interest rates, financial statements belonging to the corporate sector, trading volumes and investment decisions and economic policies which are continuously shaped by macroeconomic indicators. Despite providing unprecedented opportunities for insight, the abundant data also introduces further analytical challenges. Clean analysis is difficult without proper preprocessing because raw financial data is rarely uniform or directly comparable.\n\nData normalisation is one of the most important preprocessing steps in financial analytics. Dramatic differences in scale, unit and statistical behaviour are observed in financial variables. For example, financial ratios such as return on equity may exist on a smaller numerical scale but the revenue figures of the company may be measured in billions. Models can become biased towards large magnitude variables while analysing without adjustment, despite the actual importance of the variables.\n\nWhether financial data is transformed into a consistent or comparable format or not is ensured by normalisation. This allows more focus of the analytical models on meaningful relationships apart from numerical dominance. The importance of normalising financial data, raw datasets and their challenges, popular normalisation techniques and how financial analysis and decision-making are impacted by these methods are discussed in the given article.\n\n### IMBALANCE OF SCALE AND NATURE OF FINANCIAL DATA:\n\nFinancial datasets are characterized by intrinsic variability. Collection of market data, accounting figures and economic indicators happen from different sources which are further reported under different conventions. Wide variation of prices is observed across several asset, dramatic fluctuations of trading volumes are observed between securities and entirely different distributions are often followed by balance sheet items.\n\nProblems in both machine learning and statistical models arise from this kind of scale imbalance. The variables with smaller numerical ranges are often shadowed by the ones having large numerical ranges, despite the former having significant informational value. For instance, volatility or liquidity measures may be dominated by raw market capitalization which further leads to results which are asymmetric.\n\nThis imbalance is corrected by rescaling data into a common framework through normalisation. The appropriate contribution of the variable to analysis, which further improves accuracy, fairness and interpretability across financial models is ensured by it.\n\n### RAW FINANCIAL DATASETS AND ITS COMMON CHALLENGES:\n\nNoise and inconsistencies are frequently observed in raw financial data. Common issues include reporting delays, differences in currencies, values which are missing varying fiscal calendars. Sudden spikes caused by announcements of earning, unexpected economic events and regulatory changes are also included in market data.\n\nMajor challenges are posed by outliers. Averages and variances can be distorted, or analytical results can be misleading due to extreme price movements and unusually high trading volumes. Inflation, evolution of accounting standards and structural market changes affect financial data that can be utilized in the long run.\n\nThe reliability of analysis is significantly reduced by these issues without proper preparation of data and normalisation. These problems are mitigated by improving comparability across time periods and assets and stabilising distributions through clean, normalised data.\n\n### OVERVIEW OF FINANCIAL NORMALISATION TECHNIQUES:\n\nFinancial analysis involves the application of several normalisation techniques, each of which serve a different purpose. The values are rescaled into a fix range, typically between zero and one through the Min-max normalisation. This method helps analysts preserve relative differences while ensuring comparability.\n\nBased on mean and standard deviation, data can be transformed using Z score standardisation. The deviation of the values from historical norms are highlighted int this approach and it is also used in analysing returns, performance metrics and risk factors. To manage exponential growth and reduce skewness, market capitalization, price series and volume data often use logarithmic transformations. To limit the influence of extreme outliers, in some cases, robust scaling techniques are recommended.\n\nThe assumptions of the analytical model, the nature of the data and the objectives of the analysis depend on the techniques chosen.\n\n###  NORMALISATION’S ROLE IN FINANCIAL MODELING:\n\nAn important role is played by normalisation in financial modelling. The stability of regression coefficients is improved, and the interpretability of results is enhanced in statistical models. Convergence is sped up through normalised data which further removes disproportionately waiting, large scale features from the models of machine learning algorithms.\n\nFair comparison across assets having volatile profiles and different price levels is allowed by normalised inputs during portfolio construction. The consistent evaluation of behavioural indicators and transaction values are ensured by normalisation in fraud detection models and credit risk.\n\nBoth the robustness and accuracy of the financial models are enhanced by normalisation, which further increases their reliability in real world applications.\n\n### THREATS ASSOCIATED WITH IGNORING NORMALISATION:\n\nSerious analytical errors are consequences of failure to normalise data. Hidden biases in the input data can lead to the failure of the models in the real market despite the models performing apparently well while training. Unknowingly relying on distorted signals by decision makers may lead to scale dominance, not economic reality.\n\nFinancial losses and poor execution are consequences of rapid propagation of such errors, especially in automated, high frequency trading systems. Confidence in data driven strategies are reduced over time due to repeated in accuracies that undermine trust in analytical systems.\n\n### LIMITATIONS AND PRACTICAL CONSIDERATIONS:\n\nThoughtful application of normalisation is necessary despite it being essential. Particularly during periods of market stress, rapid changes in financial data distributions are observed over time. Normalised strategies working in stable conditions need to be adjusted especially during volatile phases.\n\nInterpretability is another important consideration. Results become difficult to be explained to stakeholders due to excessive transformation, especially in regulated environments where maintaining transparency is a big task. Mathematical precision with practical clarity must be balanced by analysts.\n\n### NORMALISED DATA IN MODERN FINANCIAL ANALYTICS AND ITS IMPORTANCE:\n\nThe need for consistent and clean data grows stronger as financial analytics become more advanced. Applications such as risk management, algorithmic trading, financially intelligent platforms and forecasting are supported by normalised data.\n\nThe quality of insights is improved, and better strategic decisions are supported by normalisation, which further enables fair comparisons across markets, time horizons and assets. Simply, normalisation is a bridge between meaningful financial understanding and raw data.\n\n### CONCLUSION:\n\nNormalisation of financial data is a core requirement for accurate and clean analysis, despite it being a secondary step. Scale differences, distortions and inconsistencies present in raw datasets can mislead analytical models if not properly dealt with. Analysts can reduce bias, improve model performance and discover genuine patterns with the data through appropriate normalisation. Effective data normalisation will remain a cornerstone of trustworthy and reliable financial analysis, as finance continues to evolve into a data-driven field."
    },
    {
        "title": "A Review of Introduction to Kolmogorov-Arnold Network(KAN)",
        "excerpt": "In the past 60 years Multi-Layer Perceptron(MLP) has helped us as the fundamental block of deep learning based on the approximation theorem. Though MLP suffered from a critical lacking of interpretability and require massive parameter often. This article explores Kolmogorov-Arnold Network(KAN) model...",
        "date": "Dec 25, 2025",
        "category": "Backend Engineering",
        "author": "Arpan Pal",
        "slug": "an-introduction-to-kolmogorov-arnold-network-kan",
        "content": "### Abstract\n\nIn the past 60 years Multi-Layer Perceptron(MLP) has helped us as the fundamental block of deep learning based on the approximation theorem. Though MLP suffered from a critical lacking of interpretability and require massive parameter often. This article explores Kolmogorov-Arnold Network(KAN) model which was proposed in 2024. This Model was proposed by Andrey Kolmogorov and Vladimir Arnold. Previously MLP placed Fixed Activation functions on nodes but KAN Place Learnable Activation functions on edges. Here we will analyse the mathematical difference between these two architectures by examining the use B-Splines on edge and weights and we will also discuss How KAN offered us promised path towards the \"White Box\" AI that is very much accurate and interpretable.\n\n---\n\n### Introduction\n\n**1. Stagnation of Perceptron**\nThese neural network is built on a mathematical assumption specifically that is the linear combination of Inputs Based on Fixed nonlinearity. In standard MLP, we define a neural computation as:\n$$y = \\sigma \\left( \\sum_{i=1}^n w_i x_i + b \\right)$$\n**Here:**\n*   $w_i$ Indicates Learnable Scalar weights.\n*   $\\sigma$ Indicates Fixed activation function.\n\nThese structure Took help From the activation function by making it a static Gatekeeper. This structure results In Black box while universally approximating where Specific contribution of features is calculated by layers of matrix multiplication.\n\n---\n\n**2. Mathematical Foundation**\nTo understand KAN We focused 2 fundamental mathematical theorem.\n*   **Universal Approximation Theorem (MLP Basis):** MLP Calculates that Sum of all nonlinear function can give an approximate result to any continuous functions.\n*   **Kolmogorov-Arnold Theorem (KAN Basis):** KAN states that any multivariate continuous function lie on a bounded domain can be represented as a finite composition of multiple univariate functions.\n$$f(\\mathbf{x}) = f(x_1, \\dots, x_n) = \\sum_{q=0}^{2n} \\Phi_q \\left( \\sum_{p=1}^n \\phi_{q,p}(x_p) \\right)$$\n**In this formula:**\n*   $\\phi_{q,p}$ are univariate functions (functions of a single variable).\n*   $\\Phi_q$ is the aggregation function.\n\n---\n\n### Architecture\nThe first innovation on KAN is by replacing the linear weight with a learnable non-linear function.\n*   **The Activation Age:** In MLP if an `age = X` and `Skills Scales = W`. Then in KAN an `age = X` and transforms it via a function $\\phi(x)$.\n*   **Basic Splines(B-Splines) Implementation:** We cannot learn from an arbitrary function directly from code, so we will approximate these functions by using B-Splines, which are piecewise polynomial functions defined by control points.\nThe learnable function $\\phi(x)$ is expressed as:\n$$\\phi(x) = \\sum_i c_i B_i(x)$$\n**Where:**\n*   $B_i(x)$ are the fixed basis functions (the spline shape).\n*   $c_i$ are the learnable coefficients (control points).\n\n---\n\n### Pseudocode\n```python\n# 2. The KAN Approach (Learnable Edges, Summing Nodes)\nclass KAN_Layer:\n    def forward(self, x):\n        # Step A: Apply Learnable Non-Linear Functions\n        # Instead of multiplying by a scalar 'W', we apply a function 'phi'\n        # 'phi' is built using B-Splines (basis functions)\n        \n        # logic: y = sum(phi(x))\n        \n        # We compute the shape of the function based on learnable coefficients\n        spline_basis = compute_b_splines(x)\n        phi_x = spline_basis * self.spline_coefficients\n        \n        # Step B: Summation\n        # The node simply sums up the incoming function results\n        y = sum(phi_x)\n        return y\n```\n\n---\n\n### Advantages\n1.  **White box interpretability:** We know that every edge is a univariate function that we can see exactly how one input variable affects the other output variable. In MLP Feature interactions are ignored in weight matrices, but KAN allows us to plot one spline shape on every connection. For example, if spline shapes look like a parabola we confirm it is a quadratic relationship; if it shapes like a sinusoidal then we confirm the relationship is periodic.\n2.  **Parameter efficiency:** In Symbolic Regression KAN helps by reaching lower loss values with fewer parameters than MLPs. A small KAN often discovers a physics law where a large MLP only poorly approximates the curve without understanding its structure.\n\n---\n\n### Conclusion\nKAN shows a paradigm shift from learning linear weights to learning functions by using the Kolmogorov-Arnold Theorem and B-Splines. KAN offers us a glimpse into the future of interpretable Artificial Intelligence. Though currently training slower than MLPs due to the complexity of calculating splines, they provide a bridge over the gap between deep learning and symbolic mathematics. For an AI engineer studying KAN will help rethink the atomic unit of neural computation."
    },
    {
        "title": "The \"Zero-API\" Financial Stack: Building a Bloomberg-Lite without the $24k Price Tag",
        "excerpt": "The whole financial world is based on one very important thing that is free and fair news. We aimed to provide our users with authentic financial news without the $24k Bloomberg API cost by using advanced scraping and Google News RSS.",
        "date": "Dec 28, 2025",
        "category": "Financial Analysis",
        "author": "Avigyan Das",
        "slug": "zero-api-financial-stack-bloomberg-lite",
        "content": "#### abstract:\n\nThe whole financial world is based on one very important thing that is free and fair news we aimed to provide our  users with news all cross domains sectors each and everything that affect the financial market and the stocks directly but getting a  source of this news was a problem the most prominent source of news the **Bloomberg api** has a huge cost thats **$24k** an impossible amount to invest as a small student research group who are just trying to get into developing new and innovative web app. The solve was rather interesting we used scraping along with **google news RSS** that too was noisy so we custom made a strict query to get each and every detailed relevant news.\n\n\n\n#### body:\n\n\n\n\n\n###### The Problem:\n\nInnovation initiative is a student group of 5 engineering students in 3rd year we specialize in AI\\&ML we wanted to make a smart web app low cost but that really works on authentic data so we figured out making a stock news sentiment analysis app was our best bet we set out to make the app from scratch without prior knowledge of how to make it what kind of dashboard to use even what to do to start our project we endured and made the dashboard a ui we were happy about now came the challenge we had  a nice dashboard a working sentiment meter that we tested on manually added dummy news but we didn't have the main key ingredient the real time authentic news. Through research a bit of help from google  and other sources here and there we found out the best source is the Bloomberg api that's the biggest source of the news but that's when we realized that every good thing has a equally high cost the cost of api $24000\n\nafter this we felt that the feat of adding Realtime news is out of question we even though of creating section where we manually add news everyday but all this failed as its not humanly possible to maintain news of whole world.\n\n\n\n\n\n \n\n###### The Solution:\n\nThe solution we got was closely related to one of out 2nd year project where we directly scraped data from the webpage and use the information so we implemented that data scraping we scraped data directly from the xml of webpage we also applied advanced rss query to stop the noisy bad news that are irrelevant now we had the perfect news source that we could freely use to get the market or a specific stock's vibe score.\n\n\n\nThe news was good but still it was lacking it needed to be refined more so we applied the rss query properly into the news fetch \n\nbelow given is the rss query we used to get proper news that's usable:\n\n\n\n```typescript\n\nfunction getSearchUrl(query: string, days: string, strict: boolean) {\n\n   const encodedQuery = encodeURIComponent(query);\n\n   const timeFilter = `when:${days}`; // e.g., '7d' for 7 days\n\n   \n\n   // In strict mode, we force financial context\n\n   if (strict) {\n\n      return `https://news.google.com/rss/search?q=${encodedQuery}+stock+finance+${timeFilter}\\&hl=en-IN`;\n\n   }\n\n   return `https://news.google.com/rss/search?q=${encodedQuery}...`;\n\n}\n\n```\n\nin the code block that we have added we can see that we forced the use of keywords like \"stock\", \"finance\", etc. and then we used 'when:' operator using this we have filtered out 90% of nonrelevant noisy news even if a very fer noisy news are left the vibe score algorithm isn't affected by those as we have added more keyword filters to stop the excess or wrongly scoring the news \n\n\n\n##### Conclusion:\n\nThe web scraping along with the rss query is a simple yet very useful tool for small developers and student researchers who cant afford costly api we successfully implemented it in an working webapp that is very accurate and useful we aim to make improvements to this \n\nstay tuned to our blogs as we aim to provide further details about both our own projects, researches as well as topics that are related neural network and our field of study"
    },
    {
        "title": "The Science behind designing websites for the mass mind",
        "excerpt": "Designing a website for a huge amount of public involves many challenges. This article focuses on technical and psychological foundations required to build a high traffic intensive interface.",
        "date": "Dec 28, 2025",
        "category": "UI/UX Design",
        "author": "Avijit",
        "slug": "science-behind-designing-websites-mass-mind",
        "content": "### Abstract\n\nDesigning a website for a huge amount of public involves many challenges. When a website is designed for millions of publics, the developer needs to take care of certain things such as technical literacy of users, capability of user’s device and various other problems which the user may face during the website navigation. Imagine a situation where a parent needs to pay their bill online in the app/website in a crowded place while holding their crying baby. It can frustrate the user if the app/website is not cognitively efficient. This article focuses on technical and psychological foundations required to build a high traffic intensive interface. It mainly focuses on the Cognitive Load theory and “Human Computer Interaction” principles such as Miller’s Law, Hick’s Law and Fitts’s Law to create a design which is accessible, intuitive and scalable to elevate the user’s digital experience.\n\n### Introduction\n\nWhen a developer designs for one, he needs to please only one person. When designing for a client, he needs to please the boardroom. But when designing for public – a diverse, unpredictable and massive number of users, the developer needs to please the human brain in general. The developer needs to keep in mind about the different patience level, internet speeds, device capabilities of the large number of users. If the developer needs to please the human brain, he will have to work not just a designer but act as a cognitive architect.\n\n### Explanation\n\nGiven below are some of the critical concepts and laws that the developer needs to keep in brain while designing –\n\n#### 1. Cognitive Load\n\nIt is amount of energy the human brain needs to spend to navigate the user interface. It is like a fuel needed to run a car. Better mileage equals to high user base of that car. So, in this case, developers need to think about how they can reduce the cognitive load to attract huge number of publics. There are two types of cognitive load –\n\n1.  **Intrinsic Load:** The minimum effort required to achieve a specific goal. The developer can simplify the load but cannot eliminate it completely.\n2.  **Extraneous Load:** The amount of mental effort wasted due to poor user interactive design.\n\n**Ways to fix this issue –**\n\n1.  Keep the user interface design simple and standard.\n2.  Remove any unwanted element.\n3.  Remove extraneous load completely.\n\n#### 2. Hick’s Law\n\nHick’s Law states that the more choices a person is presented with, the more time the person takes to make the decision. For a mass, this becomes critical to choose a option, if the developer gives different options at the same time. They simply become overwhelmed when they see many options, often choose nothing and simply leave.\n\n**Ways to fix this –**\n\n1.  Give only the essential elements needed.\n2.  Try to reduce the number of options if possible.\n3.  Do not show all options at once. Instead break them into steps.\n\n#### 3. Miller’s Law\n\nMiller’s law states that the working memory of average person can only keep 7 (plus or minus 2) items at a time. In this case, it means that an average human who is not able to keep a long string of numbers in their memory, they break them into small chunks.\n\n**Ways to fix this –**\n\n1.  Breaking a series of large numbers into small chunks of digestible groups.\n2.  Place all the elements into groups if possible.\n\n#### 4. Fitts’s Law\n\nFitts’s law states that the time required to move to a specific goal depends on the distance to it divided by the size of the target. We can use this law in our designs by simply putting essential elements closer to each other. Keep other non-essential elements away from essential ones.\n\n**Ways to fix this –**\n\n1.  Keep the essential elements bigger in size, predictable and close to each other. Don’t place the across screens that will fail this law.\n2.  For mobile users, place the essential items near the reach of the thumb finger. Don’t place them away from the reach of finger. A simple example is to place the search button of app drawer below the screen to make it easy for thumb to reach.\n\n#### 5. Visual Hierarchy\n\nWhen all information is given at once, users often do not read the full. They only scan some portions and may miss the important ones.\n\n*   **The F pattern:** The pages that are heavily loaded with text, the eye scans the top, skips a bit, moves down, again scans a little bit and then moves down following the left side of the page, making a F-pattern.\n*   **The Z pattern:** The pages that are heavily loaded with images, the eye scans from the top left to the top right, then moves diagonally to the middle, again scans from left to right and eventually moves at the bottom, making a Z pattern.\n\n**Ways to fix this –**\n\n1.  Put the critical value in the top left to right or in dead center.\n2.  Do not place any important value in the bottom right (blind spot) of a page.\n3.  Use heading, bullets and numbers to stop the f pattern.\n\n#### 6. Accessibility\n\nAccessibility is not just a feature; it is the fundamental. Making a website for the mass means it includes various types of personality, various internet speeds and various difficult situations in which the user may need to navigate the website.\n\n**Ways to fix –**\n\n1.  Use high contrast texts\n2.  Use elements which anyone can understand at any time.\n\n### Conclusion\n\nWhen designing a website for the mass the developer needs to get rid of his/her own preference and should have an empathy for the users using the website. By reducing cognitive load, the developer is able to reduce the mental energy needed by the user to navigate the website. By following Hick’s law, the developer can speed up users’ decision making; by following Miller’s law, users can retain important information and following the Fitts’s law and accessibility standards, users can easily navigate thought the website. The motive of the developer should be to make the website cognitively efficient rather than making it beautiful, which will ultimately result in their experience elevation and satisfaction of the user."
    },
    {
        "title": "How Statistical Data Analysis helps in driving Insights and Strategies in the feild of Stock Market",
        "excerpt": "We all know that stock market daily operates on the concept of uncertainty. It\nutilizes on volatility and huge stream of data. To overcome the uncertainty, the\nconcept of statistical data analysis has been introduced. Statistical analysis\nplays an important role in the modern stock market.",
        "date": "Dec 29, 2025",
        "category": "Financial Analysis",
        "author": "Arghadeep Saha",
        "slug": "how-statistical-data-analysis-helps-in-driving-insights-and-strategies-in-the-feild-of-stock-market",
        "content": "### Abstract\nWe all know that stock market daily operates on the concept of uncertainty. It\nutilizes on volatility and huge stream of data. To overcome the uncertainty, the\nconcept of statistical data analysis has been introduced. Statistical analysis\nplays an important role in the modern stock market. It performs various\noperations like transforming large amount of financial data into actionable\ninsights, understanding behavior of prices of stocks for optimizing the\nportfolios and managing the risk. Statistical methods enable analysts and\ninvestors to make probability driven decisions. This blog wil give a clear and\ngood understanding on how statistical techniques are performed in the field of\nstock market by highlighting the key characteristics or features, practical\nbased applications and commonly used fearures or tools that provide support\nin data-driven trading and investing strategies.\n\n### Introduction\nThe stock market is very much uncertain as no one can predict what will\nhappen in the future or in the coming days. It is influenced by various\nelements like economic based indicators, performance of finance based\ncorporate societies, worldwide events, and sentiment of investors. As huge\nnumber of trades are occurring everyday , relying solely on speculation is no\nlonger efficient. Statistical data analysis provides a well organized and\nstructured approach to interprete market baed data and reduce uncertainty in\ndecision-making process.\nBy examining the historical movement of prices, returns, and volume patterns,\nstatistics helps the traders, investors and the people who are very much\nengaged with stock market to identify the trends, measure the level of risk and\nevaluate potential results. In today’s data-driven markets, statistical data\nanalysis forms the core of technical based analysis and quantitative based\ntrading. It also forms the foundation of risk management systems.\n\n### Applications of Statistical Data Analysis in the field of Stock Market and\ntrading\n\n1. **Understanding the pattern of market**\nVarious Statistical methods like mean, variance, and standard deviation are\nwidely used for analyzing the prices of stocks and returns. These measures\nhelp the traders and investors to understand and analyze the average\nperformance and price volatility of stocks. It also helps in providing insight into\nhow much stable or risky a stock is.\n2. **Measuring the amount of Risk and Volatility associated with the market\nstocks**\nMeasuring the amount of risk, associated with a particular stock, is a critical\naspect of investing. Statistical functions such as beta, standard deviation, and\nValue at Risk (VaR) help in quantifying the potential losses. They also help in\nproviding a clear market sensitivity. These tools help the investors to evaluate\nthe amount of risk before making final decision regarding investment.\n3. **Identifying the market patterns or trends**\nTime series analysis feature helps in the study of movement of stock price\nover time. Techniques like moving of averages and analysis of trend help in\nassisting the detection of price momentum, market cycles, and possible\nreversals that create the basis of technical trading plans.\n4. **Portfolio Diversification and Optimization**\nMethods like Correlation and covariance analysis help in determining the\nrelationships between different types of stocks or asset classes. By selecting\nthe assets which are associated with weak or negative correlation, investors\nor traders can easily reduce the overall portfolio risk without significantly\naffecting the expected returns.\n5. **Forecasting and Predictive Analysis**\nNumerous Statistical models like regression analysis and ARIMA are widely\nused to predict the prices of future stocks or returns which are based on\nhistorical data. Though predictions are very much probabilistic, they help the\ninvestors in making proper strategy based plans\nThey help in managing the expectations also.\n6. **Role in Algorithmic and Quantitative Trading**\nQuantitative based trading systems are heavily relied on statistical indicators\nand probability models to study or analyze past data, uncover the market\ninefficiencies and automatically execute the trades which are based on the\nreliable statistically confirmed signals.\nCommon Statistical based Tools or features which are used for analyzing the\nStock Market.\nPython – One of the Widely used programming language for analyzing the\nfinancial data, modeling, and backtesting\nR – Very much popular for statistical computation and forecasting of time\nseries\nExcel – Used for basic analysis, tracking of portfolios and regression\nMATLAB – Applied in the field of advanced financial modeling and\nsimulations.\n\n### Conclusion\n\nStatistical data analysis is the keystone of analyzing the stock market,\nallowing the market investors to interpret complex market based data with\ngreater clarity. By enabling the risk management, forecasting, and data-driven\nbased strategies, statistics helps in reducing the uncertainty in a market\nenvironment which is very much unpredictable. As technology and availability\nof data are continously evolving the stock markets, statistical data analysis\nwill always remain an essential or vital tool for an informed investment related\ndecision-making.\n\n"
    }
]